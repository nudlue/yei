<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <title>Sound Classifier Demo</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
  <style>
    body {
      font-family: sans-serif;
      text-align: center;
      padding: 40px;
      background: #f7f7f7;
    }
    button {
      padding: 12px 24px;
      font-size: 20px;
      margin: 10px;
      cursor: pointer;
    }
    #result {
      margin-top: 30px;
      font-size: 30px;
    }
    #emoji {
      font-size: 80px;
      margin-top: 10px;
    }
  </style>
</head>

<body>
  <h1>ğŸ”Š ì‹¤ì‹œê°„ ì†Œë¦¬ ë¶„ë¥˜ê¸°</h1>

  <button id="startBtn">ë…¹ìŒ ì‹œì‘</button>
  <button id="stopBtn">ë…¹ìŒ ì¢…ë£Œ</button>

  <div id="result">ëŒ€ê¸° ì¤‘...</div>
  <div id="emoji">ğŸ˜¶</div>

  <script>
    let model;
    let listening = false;
    let audioContext, analyser, microphone, dataArray, stream;
    let rafId = null;

    const labels = ["Baby Crying", "Doorbell", "Fire Alarm", "ë°°ê²½ ì†ŒìŒ"];
    const emojis = {
      "Baby Crying": "ğŸ‘¶ğŸ˜­",
      "Doorbell": "ğŸšªğŸ””",
      "Fire Alarm": "ğŸ”¥ğŸš¨",
      "ë°°ê²½ ì†ŒìŒ": "ğŸŒ«ï¸"
    };

    // 1) ëª¨ë¸ ë¡œë“œ
    async function loadModel() {
      model = await tf.loadLayersModel("model.json");
      console.log("Model loaded!");
    }

    // 2) ë…¹ìŒ ì‹œì‘
    async function startRecording() {
      if (listening) return;
      listening = true;

      audioContext = new AudioContext();
      stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      microphone = audioContext.createMediaStreamSource(stream);

      analyser = audioContext.createAnalyser();
      analyser.fftSize = 1024;
      dataArray = new Float32Array(analyser.fftSize);
      microphone.connect(analyser);

      loop();
    }

    // 3) ë…¹ìŒ ì¢…ë£Œ
    function stopRecording() {
      if (!listening) return;
      listening = false;

      if (rafId) cancelAnimationFrame(rafId);

      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }

      if (audioContext) {
        audioContext.close();
      }

      document.getElementById("result").innerText = "ë…¹ìŒì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.";
      document.getElementById("emoji").innerText = "ğŸ˜´";
    }

    // 4) ë°˜ë³µ loop
    async function loop() {
      if (!listening) return;

      analyser.getFloatTimeDomainData(dataArray);

      // Teachable Machine audio preprocessing (STFT)
      let input = tf.tensor(dataArray).slice([0], [996]); 
      input = tf.signal.stft(input, 256, 128);           
      input = tf.abs(input).expandDims(-1);              
      input = tf.image.resizeBilinear(input, [43, 232]); 
      input = input.expandDims(0);                       

      const prediction = model.predict(input);
      const scores = await prediction.data();
      const idx = scores.indexOf(Math.max(...scores));

      updateUI(labels[idx]);

      rafId = requestAnimationFrame(loop);
    }

    // 5) UI ì—…ë°ì´íŠ¸
    function updateUI(label) {
      document.getElementById("result").innerText = `ì¸ì‹ ê²°ê³¼: ${label}`;
      document.getElementById("emoji").innerText = emojis[label];
    }

    // ë²„íŠ¼ ì—°ê²°
    document.getElementById("startBtn").onclick = startRecording;
    document.getElementById("stopBtn").onclick = stopRecording;

    loadModel();
  </script>
</body>
</html>
